 ## **Gradient Descent Algorithms**
 
  #### Gradient descent (GD) is a first-order iterative optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a cost function as far as possible.
      
 ## Here is my implementation.  

 ## 1- Practical Session 1
       1-1 Implement Linear Regression with single feature (variable).  
       1-2 Implement Multivariate Linear Regression.
       1-3 Implement Linear Regression using sklearn.
     
       
## 2- Practical Session 2
      2-1 Implement The Batch Gradient Descent.  
      2-2 Implement The Mini-Batch Gradient Descent.
      2-3 Implement The Stochastic Gradient Descent.
     
## 3- Practical Session 3
 Implement the accelerated gradient descent methods.
 
      3-1 Momentum – Based Batch Gradient Descent.  
      3-2 Nesterov Accelerated –based Batch Gradient Descent.
      
## 4- Practical Session 4
 Implement the accelerated gradient descent methods with adaptive learning rate. 
 
      4-1 Batch Gradient Descent & Adagrad.  
      4-2 Batch Gradient Descent & RMSProp.
      4-3 Batch Gradient Descent & Adam.


 
