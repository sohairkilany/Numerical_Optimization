 ## **Gradient Descent Algorithms**
      Gradient descent (GD) is a first-order iterative optimization algorithm that's used when training a machine learning model.
      It's based on a convex function and tweaks its parameters iteratively to minimize a cost function as far as possible.
 ## 1- Practical_1
       1-1 Implement Linear Regression with single feature (variable).  
       1-2 Implement Multivariate Linear Regression.
       1-3 Implement Linear Regression using sklearn.
       
## 2- Practical_2
      2-1 Implement The Batch Gradient Descent.  
      2-2 Implement The Mini-Batch Gradient Descent.
      2-3 Implement The Stochastic Gradient Descent.
     
## 3- Practical_3
 Implement the accelerated gradient descent methods.
      3-1 Momentum – Based Batch Gradient Descent.  
      3-2 Nesterov Accelerated –based Batch Gradient Descent.
      
## 4- Practical_4
 Implement the accelerated gradient descent methods with adaptive learning rate.  
      4-1 Batch Gradient Descent & Adagrad.  
      4-2 Batch Gradient Descent & RMSProp.
      4-3 Batch Gradient Descent & Adam.


 
